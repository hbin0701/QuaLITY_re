{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:50:44.310925Z",
     "start_time": "2023-01-17T13:50:39.336688Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This python file processes sentences.\n",
    "'''\n",
    "\n",
    "import spacy\n",
    "import jsonlines\n",
    "\n",
    "def get_sent_data(raw_text, clean_text=True):\n",
    "    \"\"\"Given a passage, return sentences\"\"\"\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"tagger\", \"lemmatizer\", \"attribute_ruler\"])\n",
    "    sent_data = []\n",
    "    raw_text = raw_text.replace('\\n', '')\n",
    "    raw_text_list = [str(sent_obj).strip() for sent_obj in nlp(raw_text).sents] \n",
    "    raw_text_list = unify_sentences([x for x in raw_text_list if len(x) > 1])\n",
    "    sent_data = ' [CLS] '.join(raw_text_list)\n",
    "    return sent_data\n",
    "\n",
    "def unify_sentences(sents, length=10):\n",
    "    \"\"\"unifies short sentences.\"\"\"\n",
    "    i = 0\n",
    "    while i < len(sents) - 1:\n",
    "        if len(sents[i]) < length or len(sents[i+1]) < length:\n",
    "            sents[i] = sents[i] + \" \" + sents[i+1]\n",
    "            sents.pop(i+1)\n",
    "        else:\n",
    "            i += 1\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:50:44.315589Z",
     "start_time": "2023-01-17T13:50:44.313099Z"
    }
   },
   "outputs": [],
   "source": [
    "# import jsonlines\n",
    "# from tqdm.auto import tqdm\n",
    "# import json\n",
    "# import os\n",
    "\n",
    "# for mode in [\"train\", \"dev\"]:\n",
    "#     jsonl_file = f'/workspace/quality/data/v1.0.1/QuALITY.v1.0.1.htmlstripped.{mode}'\n",
    "#     with jsonlines.open(jsonl_file, 'r') as jsonl_f:\n",
    "#         lst = [obj for obj in jsonl_f]\n",
    "#         results = []\n",
    "#         for file in tqdm(lst):\n",
    "#             context = get_sent_data(file['article'])\n",
    "#             for question in file['questions']:\n",
    "#                 temp = {}\n",
    "#                 temp['context'] = context\n",
    "#                 temp['question'] = question['question']\n",
    "#                 temp['options'] = question['options'] \n",
    "#                 temp['label'] = question['gold_label']\n",
    "#                 results.append(temp)\n",
    "                \n",
    "#         # write to jsonlines file\n",
    "#         with open(f'../data/{mode}.jsonl', 'w') as outfile:\n",
    "#             for d in results:\n",
    "#                 outfile.write(json.dumps(d) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:51:08.565230Z",
     "start_time": "2023-01-17T13:50:44.317358Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-50929fa4e6cf828c\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-50929fa4e6cf828c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793d545cb8fb4b21a7d65e1f0d078715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-218a2199685916a9\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-218a2199685916a9/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef0bdaf38544840b4142c00e1f303d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "############### TRAINING CODE ################\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import jsonlines \n",
    "from datasets import load_dataset\n",
    "\n",
    "# Option 1. Divide into 512 tokens, and just do it.\n",
    "# Option 2. Add Sentence Special Tokens like [CLS], and do something\n",
    "# Option 3. Add Paragraph Special Token i.e. in front of every 512 tokens.\n",
    "\n",
    "# Current Implementation: Option 1.\n",
    "model_name = 'roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Build Dataset\n",
    "dset = load_dataset('json', data_files=\"/workspace/new_quality/data/train.jsonl\")\n",
    "dset['valid'] = load_dataset('json', data_files=\"/workspace/new_quality/data/dev.jsonl\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:51:08.572207Z",
     "start_time": "2023-01-17T13:51:08.567396Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_2d_list(elem, chunk_size, pad_id):\n",
    "    \"\"\"converts tokens into chunks of 512.\"\"\"\n",
    "\n",
    "    lst = elem[\"input_ids\"]\n",
    "    att_lst = elem[\"attention_mask\"]\n",
    "\n",
    "    num_lists = len(lst) // chunk_size\n",
    "    remainder = len(lst) % chunk_size\n",
    "    two_d_list = [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)][:num_lists]\n",
    "    if remainder:\n",
    "        last_list = lst[-remainder:] + [pad_id] * (chunk_size - remainder)\n",
    "        two_d_list.append(last_list)\n",
    "\n",
    "    # Need to check!\n",
    "    two_d_att_list = [[1 for _ in range(chunk_size)] for i in range(num_lists)]\n",
    "    if remainder:\n",
    "        last_list = [1 for _ in range(chunk_size - remainder)] + [0 for _ in range(remainder)]\n",
    "        two_d_att_list.append(last_list)\n",
    "\n",
    "    return {\"context_input_ids\": two_d_list, \"context_attention_mask\": two_d_att_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:51:08.652781Z",
     "start_time": "2023-01-17T13:51:08.573892Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-50929fa4e6cf828c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-44f01b7ca2f26374.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-218a2199685916a9/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-35dc6f316b9bf2df.arrow\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def custom_tokenize(ex):\n",
    "    # CONTEXT, Make it into chuncks of 512\n",
    "    text = ex['context'].replace(\" [CLS] \", \" \")\n",
    "    result = convert_to_2d_list(tokenizer(text), chunk_size=512, pad_id=tokenizer.pad_token_id)\n",
    "\n",
    "    for k in result:\n",
    "        ex[k] = result[k]\n",
    "\n",
    "    # Question options.\n",
    "    q = tokenizer(ex['question'], padding='max_length', max_length=128)\n",
    "    o = tokenizer(ex['options'], padding='max_length', max_length=128)\n",
    "    \n",
    "    for k in q:\n",
    "        ex[\"qo_\" + k] = torch.cat([torch.Tensor([q[k]]), torch.Tensor(o[k])])\n",
    "\n",
    "    ex['label'] = ex['label'] - 1\n",
    "    return ex\n",
    "\n",
    "# Tokenize text in each dataset.\n",
    "for mode in dset:\n",
    "    dset[mode] = dset[mode].map(lambda x: custom_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:51:08.663300Z",
     "start_time": "2023-01-17T13:51:08.654791Z"
    }
   },
   "outputs": [],
   "source": [
    "# Divide into 512 tokens.\n",
    "import torch.nn as nn\n",
    "from CoCA import CrossAttention\n",
    "import pdb\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        # [N, 512]\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # [5, M]\n",
    "        self.post_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        self.ca_q = CrossAttention(dim=768)\n",
    "        self.ca_a = CrossAttention(dim=768)\n",
    "        self.mhatt = nn.MultiheadAttention(embed_dim=768, num_heads=8)\n",
    "\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(768, 768),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(768, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, context_input_ids, context_attention_mask, qo_input_ids, qo_attention_mask):\n",
    "        \"\"\"context: [N, 512], question_options: [5, M]\"\"\"\n",
    "        context = self.model(input_ids=context_input_ids, attention_mask=context_attention_mask)['last_hidden_state']\n",
    "        query = self.post_model(input_ids=qo_input_ids, attention_mask=qo_attention_mask)['last_hidden_state']\n",
    "\n",
    "        # Question = [1, A, 768]\n",
    "        # Options = [4, A, 768]\n",
    "        question = query[0,].unsqueeze(0)\n",
    "        options = query[1:,]\n",
    "\n",
    "        # Cross attention, Output: [1, 512, 768]\n",
    "        att_context = self.ca_q(context, question)\n",
    "        att_context = att_context.mean(dim=0).unsqueeze(0)\n",
    "                \n",
    "        # Option: [4, 128, 768] # attention: [1, 512, 768]\n",
    "        result = self.ca_a(att_context, options) # => [4, 512, 768]\n",
    "        result = result.mean(dim=1) # Mean Pooling => [4, 768]\n",
    "        result = self.nn(result).squeeze(-1) # [4, 768]\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:51:08.671727Z",
     "start_time": "2023-01-17T13:51:08.664739Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:51:08.689140Z",
     "start_time": "2023-01-17T13:51:08.674290Z"
    }
   },
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.local_rank = -1\n",
    "        self.output_dir = \"/workspace/new_quality/output\"\n",
    "        self.per_gpu_train_batch_size = 8\n",
    "        self.n_gpu = 1\n",
    "        self.max_steps = -1\n",
    "        self.gradient_accumulation_steps = 8\n",
    "        self.num_train_epochs = 10\n",
    "        self.learning_rate= 3e-5\n",
    "        self.adam_betas = \"(0.9, 0.999)\"\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.weight_decay = 0\n",
    "        self.warmup_steps = 0\n",
    "        self.warmup_proportion = 0\n",
    "\n",
    "        self.fp16 = False\n",
    "        self.fp16_opt_level = \"O1\"\n",
    "        self.num_train_epochs= 10\n",
    "        self.seed = 42\n",
    "\n",
    "        self.gradient_accumulation_steps = 16\n",
    "        self.no_clip_grad_norm = True\n",
    "        self.max_grad_norm = 1.0\n",
    "\n",
    "        self.logging_steps = 100\n",
    "        self.save_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:51:09.078159Z",
     "start_time": "2023-01-17T13:51:08.691822Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "from multiprocessing.spawn import import_main_path\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    BertConfig,\n",
    "    BertForMultipleChoice,\n",
    "    BertTokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaForMultipleChoice,\n",
    "    RobertaTokenizer,\n",
    "    DebertaV2Config,\n",
    "    # DebertaV2ForMultipleChoice,\n",
    "    DebertaV2Tokenizer,\n",
    "    XLNetConfig,\n",
    "    XLNetForMultipleChoice,\n",
    "    XLNetTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='./logs/train.log', level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "def train(args, train_dataset, model, tokenizer):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    \n",
    "    if args.local_rank in [-1, 0]:\n",
    "        str_list = str(args.output_dir).split('/')\n",
    "        tb_log_dir = os.path.join('summaries', str_list[-1])\n",
    "        tb_writer = SummaryWriter(tb_log_dir)\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "\n",
    "    if args.max_steps > 0:  # XXX\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    exec('args.adam_betas = ' + args.adam_betas)\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, betas=args.adam_betas, eps=args.adam_epsilon)\n",
    "    assert not ((args.warmup_steps > 0) and (args.warmup_proportion > 0)), \"--only can set one of --warmup_steps and --warm_ratio \"\n",
    "    if args.warmup_proportion > 0:\n",
    "        args.warmup_steps = int(t_total * args.warmup_proportion)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    # if args.n_gpu > 1:\n",
    "    #     model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # # Distributed training (should be after apex fp16 initialization)\n",
    "    # if args.local_rank != -1:   # XXX\n",
    "    #     model = torch.nn.parallel.DistributedDataParallel(\n",
    "    #         model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
    "    #     )\n",
    "    \n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "    # UNTIL HERE\n",
    "\n",
    "    def evaluate_model(train_preds, train_label_ids, tb_writer, args, model, tokenizer, best_steps, best_dev_acc):\n",
    "        train_acc = simple_accuracy(train_preds, train_label_ids)\n",
    "        train_preds = None\n",
    "        train_label_ids = None\n",
    "        \n",
    "        results = evaluate(args, model, tokenizer, dset[\"valid\"])\n",
    "\n",
    "        ### UNTIL HERE ###\n",
    "\n",
    "        logger.info(\n",
    "            \"train acc: %s, dev acc: %s, loss: %s, global steps: %s\",\n",
    "            str(train_acc),\n",
    "            str(results[\"eval_acc\"]),\n",
    "            str(results[\"eval_loss\"]),\n",
    "            str(global_step),\n",
    "        )\n",
    "\n",
    "        tb_writer.add_scalar(\"training/acc\", train_acc, global_step)\n",
    "\n",
    "        for key, value in results.items():\n",
    "            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "        if results[\"eval_acc\"] > best_dev_acc:\n",
    "            best_dev_acc = results[\"eval_acc\"]\n",
    "            best_steps = global_step\n",
    "            logger.info(\"achieve BEST dev acc: %s at global step: %s\",\n",
    "                        str(best_dev_acc),\n",
    "                        str(best_steps)\n",
    "            )\n",
    "\n",
    "            output_dir = args.output_dir\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            \n",
    "            torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "            logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "            txt_dir = os.path.join(output_dir, 'best_dev_results.txt')\n",
    "            with open(txt_dir, 'w') as f:\n",
    "                rs = 'global_steps: {}; dev_acc: {}'.format(global_step, best_dev_acc)\n",
    "                f.write(rs)  \n",
    "                tb_writer.add_text('best_results', rs, global_step)\n",
    "\n",
    "        return train_preds, train_label_ids, train_acc, best_steps, best_dev_acc\n",
    "\n",
    "    def save_model(args, model, tokenizer):\n",
    "        output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        model_to_save = (\n",
    "            model.module if hasattr(model, \"module\") else model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(output_dir)\n",
    "        tokenizer.save_vocabulary(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "        logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    best_dev_acc = 0.0\n",
    "    best_steps = 0\n",
    "\n",
    "    train_preds = []\n",
    "    train_label_ids = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.zero_grad()\n",
    "    train_iterator = range(int(args.num_train_epochs))\n",
    "    set_seed(args)  # Added here for reproductibility\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(dset[\"train\"], desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "\n",
    "            # by default, run evaluation ONLY in device 0.\n",
    "            inputs = {}\n",
    "\n",
    "            for k in [\"context_input_ids\", \"context_attention_mask\", \"qo_input_ids\", \"qo_attention_mask\"]:\n",
    "                inputs[k]= torch.LongTensor(batch[k]).to(device)\n",
    "            \n",
    "            label = torch.LongTensor([batch['label']]).to(device)\n",
    "            outputs = model(**inputs)\n",
    "                    \n",
    "            loss = criterion(outputs.unsqueeze(0), label)\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "            \n",
    "            train_preds = np.append(train_preds, outputs.argmax(dim=-1).detach().cpu().numpy())\n",
    "            train_label_ids = np.append(train_label_ids, label.detach().cpu().numpy())\n",
    "            \n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                if not args.no_clip_grad_norm:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "            else:\n",
    "                loss.backward()\n",
    "                if not args.no_clip_grad_norm:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            epoch_iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    train_preds, train_label_ids, train_acc, best_steps, best_dev_acc = evaluate_model(train_preds, train_label_ids, tb_writer, args, model, tokenizer, best_steps, best_dev_acc)\n",
    "                    tb_writer.add_scalar(\"training/lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"training/loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logger.info(\n",
    "                        \"Average loss: %s, average acc: %s at global step: %s\",\n",
    "                        str((tr_loss - logging_loss) / args.logging_steps),\n",
    "                        str(train_acc),\n",
    "                        str(global_step),\n",
    "                    )\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    save_model(args, model, tokenizer)\n",
    "            if args.max_steps > 0 and global_step > args.max_steps: # XXX\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps: # XXX\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        train_preds, train_label_ids, train_acc, best_steps, best_dev_acc = evaluate_model(train_preds, train_label_ids, tb_writer, args, model, tokenizer, best_steps, best_dev_acc)\n",
    "        save_model(args, model, tokenizer)\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step, best_steps\n",
    "    \n",
    "\n",
    "### EVALUATE ###\n",
    "def evaluate(args, model, tokenizer, eval_dataset, prefix=\"\", test=False,):\n",
    "\n",
    "    eval_output_dir = args.output_dir\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    results = {}\n",
    "    os.makedirs(eval_output_dir, exist_ok=True)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "\n",
    "    for batch in tqdm(eval_dataset, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "\n",
    "        # by default, run evaluation ONLY in device 0.\n",
    "        inputs = {}\n",
    "\n",
    "        for k in [\"context_input_ids\", \"context_attention_mask\", \"qo_input_ids\", \"qo_attention_mask\"]:\n",
    "            inputs[k]= torch.LongTensor(batch[k]).to(device)\n",
    "        \n",
    "        label = torch.LongTensor([batch['label']]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "                \n",
    "        loss = criterion(outputs.unsqueeze(0), label)\n",
    "        eval_loss = loss.item()\n",
    "        \n",
    "        preds = np.append(preds, outputs.argmax(dim=-1).detach().cpu().numpy())\n",
    "        out_label_ids = np.append(out_label_ids, label.detach().cpu().numpy())\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # FROM HERE.\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    acc = simple_accuracy(preds, out_label_ids)\n",
    "\n",
    "    result = {\"eval_acc\": acc, \"eval_loss\": eval_loss}\n",
    "    results.update(result)\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, \"is_test_\" + str(test).lower() + \"_eval_results.txt\")\n",
    "\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(str(prefix) + \" is test:\" + str(test)))\n",
    "\n",
    "        if not test:    \n",
    "            for key in sorted(result.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "    if test:\n",
    "        return results, preds\n",
    "    else:\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-17T13:50:42.979Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Iteration:   0%|          | 0/2523 [00:00<?, ?it/s]<ipython-input-9-fec837cb6f69>:191: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  inputs[k]= torch.LongTensor(batch[k]).to(device)\n",
      "Iteration:  18%|█▊        | 457/2523 [01:51<08:38,  3.99it/s, loss=0.0838]"
     ]
    }
   ],
   "source": [
    "args = Args()\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = Network().to(device)\n",
    "train(args, dset[\"train\"], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
